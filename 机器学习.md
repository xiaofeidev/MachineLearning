# 绪论

## 引言

【机器学习】致力于研究如何通过计算的手段，利用经验来改善系统自身的性能

计算机系统中，经验通常以【数据】形式存在

机器学习所研究的主要内容，是关于在计算机上从数据中产生【模型】的算法，即【学习算法】(【模型】泛指从数据中学得的结果)

可以说机器学习是研究关于【学习算法】的学问

## 基本术语

**数据** -> 进行机器学习，要先有数据

**记录** -> 数据集中的一条记录

**数据集** -> 一组记录的集合

**(示例/样本)** -> 数据集中的每条记录是关于一个事件或对象的描述

**(属性/特征)** -> 反映事件或对象在某方面的表现或性质的事项

**属性值** -> 喵

**(属性空间/样本空间/输入空间)** -> 属性张成的空间，如将三个属性作为三个坐标轴，张成的一个三维空间

**特征向量** -> 空间中的每个点对应一个坐标向量，因此我们也把一个示例称为一个特征向量

**维数** -> 样本的属性的个数

**(学习/训练)** -> 从数据中学得模型的过程

**训练数据** -> 训练过程中使用的数据

**训练样本** -> 训练数据中的每个样本

**训练集** -> 训练样本的集合

**假设** -> 学得的模型对应了关于数据的某种潜在的规律

**(真相/真实)** -> 潜在规律本身

**学习器** -> 模型的别称，可看作学习算法在给定数据和参数空间上的实例化

**标记** -> 训练样本示例的结果信息

**样例** -> 拥有了标记信息的训练样本示例

**(标记空间/输出空间)** -> 所有标记的集合

**分类** -> 若要预测的是**离散值**(例如好瓜、坏瓜)，此类学习任务称为分类

**回归** -> 若要预测的是连续值(例如西瓜成熟度 0.95)，此类学习任务称为回归

**二分类** -> 只涉及两个类别的分类任务，称其中一个为【正类】，另一个为【反类】

**多分类** -> 涉及多个类别的分类任务

**测试** -> 学得模型后，使用其进行预测的过程

**测试样本** -> 在测试中被预测的样本

**聚类** -> 将训练集中的数据分成若干组

**簇** -> 每组称为一个簇，这些簇可能对应一些潜在的概念划分

**监督学习** -> 训练数据有标记信息，【分类】和【回归】是其代表

**无监督学习** -> 训练数据无标记信息，【聚类】是其代表

**泛化能力** -> 学得模型适用于新样本的能力，获得具有强泛化能力的模型是机器学习的目标

一般而言，训练样本越多，越有可能获得具有强泛化能力的模型

## 假设空间

【归纳】与【演绎】是科学推理的两大基本手段

【归纳】-> 从特殊到一般的【泛化】过程

【演绎】-> 从一般到特殊的【特化】过程

从样例中学习，显然是一个归纳的过程，因此亦称【归纳学习】

【归纳学习】有狭义与广义之分，广义的归纳学习大体相当于从样例中学习

狭义的归纳学习则要求从训练数据中学得概念，因此亦称[概念学习/概念形成]

概念学习中最基本的是布尔概念学习，即对[是/不是]这样可表示为[0/1]布尔值的目标概念的学习

我们可以把学习过程看作一个在所有【假设】组成的空间中进行搜索的过程，搜索目标是找到与训练集匹配的假设

搜索过程中可以不断删除与正例不一致的假设和与反例不一致的假设，最终将会获得与训练集一致的假设，这就是我们学得的结果

【版本空间】-> 可能有多个假设与训练集一致，即存在着一个与训练集一致的[假设集合]，称之为【版本空间】

## 归纳偏好

通过学习得到的模型对应了假设空间中的一个假设

有时会有多个与训练集一致的假设

与不同假设对应的模型在面对新样本时会产生不同的输出，此时应该采用哪一个模型(或假设)呢？

有时无法判断多个假设中哪一个更好，而一个具体的学习算法必须要产生一个模型

这时，学习算法本身的【偏好】就会起到关键的作用

譬如说有的算法喜欢[尽可能特殊]的模型，有的算法喜欢[尽可能一般]的模型

机器学习算法在学习过程中对某种类型假设的偏好，称为【归纳偏好】或简称【偏好】

**任何一个有效的机器学习算法必有其归纳偏好**，否则会无法产生确定的学习结果，学得的模型也会具有较大的随机性

【奥卡姆剃刀】是一种常用的、自然科学研究中最基本的原则

于此处，即【若有多个假设与观察一致，则选最简单的那个】，可以此为一般性的原则来引导算法确立“正确的”偏好

【奥卡姆剃刀】并非唯一可行的原则，且其本身也具有局限性，有时会很难判断哪个假设更简单

算法的归纳偏好是否与问题本身匹配，大多数时候直接决定了算法能否取得好的性能

**任何学习算法，其在某些问题上表现得更好，则必然在另一些问题上表现得更差**

【没有免费午餐】定理(NFL 定理) -> **无论学习算法 a 多聪明，学习算法 b 多笨拙，他们的期望性能相同**

当然这个定理有个重要前提：所有问题出现的机会相同，或所有问题同等重要

但实际情形并非如此

我们一般只关注自己正在试图解决的具体问题，希望为它找到一个最适合的解决方案

NFL 定理最重要的寓意，是让我们清楚地认识到，脱离具体问题，空泛地谈论“什么学习算法最好”毫无意义，因为若考虑所有潜在的问题，则所有学习算法都一样好

必须针对具体的学习问题讨论学习的算法的相对优劣

学习算法自身的归纳偏好与问题是否相配，往往会起决定性的作用

## 发展历程

略

## 应用现状

略

## 阅读材料

本节主要介绍了一些机器学习领域重要的学术会议和期刊
